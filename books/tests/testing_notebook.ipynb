{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/fishy/OneDrive/Desktop/Classes/Grad_Semester_5/DS_5111/bws5dk_DS5111su24_lab_01/books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_cleantext import test_clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:tokenizers:Text cleaned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but the raven sitting lonely on the placid bust spoke only that one word as if his soul in that one word he did outpour\n"
     ]
    }
   ],
   "source": [
    "test_clean_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_tokenizer import test_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenizers:Text getting cleaned\n",
      "INFO:tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:tokenizers:Text cleaned\n",
      "INFO:tokenizers:Text has been cleaned\n",
      "INFO:tokenizers:Tokenizing words into list\n",
      "INFO:tokenizers:Text has been tokenzied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'the', 'raven', 'sitting', 'lonely', 'on', 'the', 'placid', 'bust', 'spoke', 'only', 'that', 'one', 'word', 'as', 'if', 'his', 'soul', 'in', 'that', 'one', 'word', 'he', 'did', 'outpour']\n"
     ]
    }
   ],
   "source": [
    "test_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_count_words import test_count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenizers:Clean and tokenize input text\n",
      "INFO:tokenizers:Text getting cleaned\n",
      "INFO:tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:tokenizers:Text cleaned\n",
      "INFO:tokenizers:Text has been cleaned\n",
      "INFO:tokenizers:Tokenizing words into list\n",
      "INFO:tokenizers:Text has been tokenzied\n",
      "INFO:tokenizers:Text is cleaned and words are in a list\n",
      "INFO:tokenizers:Counting word fequencies...\n",
      "INFO:tokenizers:Done counting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 2, 'that': 2, 'one': 2, 'word': 2, 'but': 1, 'raven': 1, 'sitting': 1, 'lonely': 1, 'on': 1, 'placid': 1, 'bust': 1, 'spoke': 1, 'only': 1, 'as': 1, 'if': 1, 'his': 1, 'soul': 1, 'in': 1, 'he': 1, 'did': 1, 'outpour': 1})\n"
     ]
    }
   ],
   "source": [
    "test_count_words()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
