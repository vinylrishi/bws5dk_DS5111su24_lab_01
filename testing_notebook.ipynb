{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Notebook for Testing Tokenizer Functions\n",
    "## Rishi Sharma, bws5dk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure path matches that of the python files and testing texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'tests'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for intentional passes on small initial string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_cleantext import test_clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but the raven sitting lonely on the placid bust spoke only that one word as if his soul in that one word he did outpour\n"
     ]
    }
   ],
   "source": [
    "test_clean_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_tokenizer import test_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'the', 'raven', 'sitting', 'lonely', 'on', 'the', 'placid', 'bust', 'spoke', 'only', 'that', 'one', 'word', 'as', 'if', 'his', 'soul', 'in', 'that', 'one', 'word', 'he', 'did', 'outpour']\n"
     ]
    }
   ],
   "source": [
    "test_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_count_words import test_count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenizers:Clean and tokenize input text\n",
      "INFO:tokenizers:Text getting cleaned\n",
      "INFO:tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:tokenizers:Text cleaned\n",
      "INFO:tokenizers:Text has been cleaned\n",
      "INFO:tokenizers:Tokenizing words into list\n",
      "INFO:tokenizers:Text has been tokenzied\n",
      "INFO:tokenizers:Text is cleaned and words are in a list\n",
      "INFO:tokenizers:Counting word fequencies...\n",
      "INFO:tokenizers:Done counting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 2, 'that': 2, 'one': 2, 'word': 2, 'but': 1, 'raven': 1, 'sitting': 1, 'lonely': 1, 'on': 1, 'placid': 1, 'bust': 1, 'spoke': 1, 'only': 1, 'as': 1, 'if': 1, 'his': 1, 'soul': 1, 'in': 1, 'he': 1, 'did': 1, 'outpour': 1})\n"
     ]
    }
   ],
   "source": [
    "test_count_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for intentional failures   \n",
    "(feel free to comment these out when running to avoid stoppage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "We succefully failed, as our output should not be the original text: But the Raven, sitting lonely on the placid bust, spoke only That one word, as if his soul in that one word he did outpour. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12640\\1116812433.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtest_cleantext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_cleantext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_clean_text_fail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\fishy\\OneDrive\\Desktop\\Classes\\Grad_Semester_5\\DS_5111\\bws5dk_DS5111su24_lab_01\\tests\\test_cleantext.py\u001b[0m in \u001b[0;36mtest_clean_text_fail\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"But the Raven, sitting lonely on the placid bust, spoke only That one word, as if his soul in that one word he did outpour.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mcleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mcleaned\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"But the Raven, sitting lonely on the placid bust, spoke only That one word, as if his soul in that one word he did outpour.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"We succefully failed, as our output should not be the original text: {text} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Our cleaned output: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: We succefully failed, as our output should not be the original text: But the Raven, sitting lonely on the placid bust, spoke only That one word, as if his soul in that one word he did outpour. "
     ]
    }
   ],
   "source": [
    "import test_cleantext\n",
    "test_cleantext.test_clean_text_fail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Clean and tokenize input text\n",
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n",
      "INFO:bws5dk.tokenizers:Text is cleaned and words are in a list\n",
      "INFO:bws5dk.tokenizers:Counting word fequencies...\n",
      "INFO:bws5dk.tokenizers:Done counting\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "We have succefully failed as `the` appears twice, not once in the test sentence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12640\\428639357.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtest_count_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_count_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_count_words_fail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\fishy\\OneDrive\\Desktop\\Classes\\Grad_Semester_5\\DS_5111\\bws5dk_DS5111su24_lab_01\\tests\\test_count_words.py\u001b[0m in \u001b[0;36mtest_count_words_fail\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"Word count failed on sample text: {text}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'the'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"We have succefully failed as `the` appears twice, not once in the test sentence\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: We have succefully failed as `the` appears twice, not once in the test sentence"
     ]
    }
   ],
   "source": [
    "import test_count_words\n",
    "test_count_words.test_count_words_fail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenizers:Text getting cleaned\n",
      "INFO:tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:tokenizers:Text cleaned\n",
      "INFO:tokenizers:Text has been cleaned\n",
      "INFO:tokenizers:Tokenizing words into list\n",
      "INFO:tokenizers:Text has been tokenzied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'the', 'raven', 'sitting', 'lonely', 'on', 'the', 'placid', 'bust', 'spoke', 'only', 'that', 'one', 'word', 'as', 'if', 'his', 'soul', 'in', 'that', 'one', 'word', 'he', 'did', 'outpour']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Failed as intended. We have 25 words, not 29",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7364\\1239369464.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtest_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_tokenize_fail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\fishy\\OneDrive\\Desktop\\Classes\\Grad_Semester_5\\DS_5111\\bws5dk_DS5111su24_lab_01\\books\\tests\\test_tokenizer.py\u001b[0m in \u001b[0;36mtest_tokenize_fail\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"Tokenizer failed on sample text: {text}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m29\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"Failed as intended. We have {len(tokens)} words, not 29\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Failed as intended. We have 25 words, not 29"
     ]
    }
   ],
   "source": [
    "import test_tokenizer\n",
    "test_tokenizer.test_tokenize_fail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for larger texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'tests'))\n",
    "import test_cleantext\n",
    "import test_tokenizer\n",
    "import test_count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = ['pg17192.txt',\n",
    "    'pg932.txt',\n",
    "    'pg1063.txt',\n",
    "    'pg10031.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the project gutenberg ebook of the raven\n",
      "    \n",
      "this ebook is for the use of anyone anywhere in the united states and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever you may copy it give it away or reuse it under the terms\n",
      "of the project gutenberg license included with this ebook or online\n",
      "at wwwgutenbergorg if you are not located in the united states\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this ebook\n",
      "\n",
      "title the raven\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cleantext.test_clean_text_raven()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffthe', 'project', 'gutenberg', 'ebook', 'of', 'the', 'raven', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of']\n"
     ]
    }
   ],
   "source": [
    "test_tokenizer.test_tokenize_raven()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Clean and tokenize input text\n",
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n",
      "INFO:bws5dk.tokenizers:Text is cleaned and words are in a list\n",
      "INFO:bws5dk.tokenizers:Counting word fequencies...\n",
      "INFO:bws5dk.tokenizers:Done counting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for word `the`: 729\n"
     ]
    }
   ],
   "source": [
    "test_count_words.test_count_words_raven()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of pg932.txt text: 60314\n",
      "Sample of pg932.txt text: ﻿the project gutenberg ebook of the fall of t\n"
     ]
    }
   ],
   "source": [
    "test_cleantext.test_clean_text_all('pg932.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of pg932.txt tokeized list: 10133\n",
      "Sample of pg932.txt tokens: \n",
      "['\\ufeffthe', 'project', 'gutenberg', 'ebook', 'of', 'the', 'fall', 'of', 'the', 'house', 'of', 'usher', 'this', 'ebook', 'is']\n"
     ]
    }
   ],
   "source": [
    "test_tokenizer.test_tokenize_all('pg932.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Clean and tokenize input text\n",
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n",
      "INFO:bws5dk.tokenizers:Text is cleaned and words are in a list\n",
      "INFO:bws5dk.tokenizers:Counting word fequencies...\n",
      "INFO:bws5dk.tokenizers:Done counting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for word `the`: 748\n"
     ]
    }
   ],
   "source": [
    "test_count_words.test_count_words_all('pg932.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of combined text after pg932.txt text added: 60315\n"
     ]
    }
   ],
   "source": [
    "test_cleantext.test_clean_text_combined('pg932.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of combined text after pg1063.txt text added: 91698\n"
     ]
    }
   ],
   "source": [
    "test_cleantext.test_clean_text_combined('pg1063.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleantext.test_delete_concat_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Text getting cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of combined texts after pg932.txt text added:: 10133\n"
     ]
    }
   ],
   "source": [
    "test_tokenizer.test_tokenize_combined('pg932.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Text getting cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of combined texts after pg1063.txt text added:: 15508\n"
     ]
    }
   ],
   "source": [
    "test_tokenizer.test_tokenize_combined('pg1063.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizer.test_delete_concat_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Clean and tokenize input text\n",
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n",
      "INFO:bws5dk.tokenizers:Text is cleaned and words are in a list\n",
      "INFO:bws5dk.tokenizers:Counting word fequencies...\n",
      "INFO:bws5dk.tokenizers:Done counting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for word `the` in combined text after pg1063.txt text added: 358\n"
     ]
    }
   ],
   "source": [
    "test_count_words.test_count_words_combined('pg1063.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Clean and tokenize input text\n",
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n",
      "INFO:bws5dk.tokenizers:Text is cleaned and words are in a list\n",
      "INFO:bws5dk.tokenizers:Counting word fequencies...\n",
      "INFO:bws5dk.tokenizers:Done counting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for word `the` in combined text after pg932.txt text added: 1106\n"
     ]
    }
   ],
   "source": [
    "test_count_words.test_count_words_combined('pg932.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count_words.test_delete_concat_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing French text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: mais le corbeau perché solitairement sur ce buste placide parla\n",
      "    ce seul mot comme si son âme en ce seul mot il la répandait je ne\n",
      "    proférai donc rien de plus il nagita donc pas de plumejusquà ce\n",
      "    que je fis à peine davantage que marmotter «dautres amis déjà ont\n",
      "    pris leur voldemain il me laissera comme mes espérances déjà ont\n",
      "    pris leur vol» alors loiseau dit «jamais plus»\n"
     ]
    }
   ],
   "source": [
    "test_cleantext.test_clean_text_french()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mais', 'le', 'corbeau', 'perché', 'solitairement', 'sur', 'ce', 'buste', 'placide', 'parla', 'ce', 'seul', 'mot', 'comme', 'si', 'son', 'âme', 'en', 'ce', 'seul', 'mot', 'il', 'la', 'répandait', 'je', 'ne', 'proférai', 'donc', 'rien', 'de', 'plus', 'il', 'nagita', 'donc', 'pas', 'de', 'plumejusquà', 'ce', 'que', 'je', 'fis', 'à', 'peine', 'davantage', 'que', 'marmotter', '«dautres', 'amis', 'déjà', 'ont', 'pris', 'leur', 'voldemain', 'il', 'me', 'laissera', 'comme', 'mes', 'espérances', 'déjà', 'ont', 'pris', 'leur', 'vol»', 'alors', 'loiseau', 'dit', '«jamais', 'plus»']\n"
     ]
    }
   ],
   "source": [
    "test_tokenizer.test_tokenize_french()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Clean and tokenize input text\n",
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n",
      "INFO:bws5dk.tokenizers:Text is cleaned and words are in a list\n",
      "INFO:bws5dk.tokenizers:Counting word fequencies...\n",
      "INFO:bws5dk.tokenizers:Done counting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'ce': 4, 'il': 3, 'seul': 2, 'mot': 2, 'comme': 2, 'je': 2, 'donc': 2, 'de': 2, 'que': 2, 'déjà': 2, 'ont': 2, 'pris': 2, 'leur': 2, 'mais': 1, 'le': 1, 'corbeau': 1, 'perché': 1, 'solitairement': 1, 'sur': 1, 'buste': 1, 'placide': 1, 'parla': 1, 'si': 1, 'son': 1, 'âme': 1, 'en': 1, 'la': 1, 'répandait': 1, 'ne': 1, 'proférai': 1, 'rien': 1, 'plus': 1, 'nagita': 1, 'pas': 1, 'plumejusquà': 1, 'fis': 1, 'à': 1, 'peine': 1, 'davantage': 1, 'marmotter': 1, '«dautres': 1, 'amis': 1, 'voldemain': 1, 'me': 1, 'laissera': 1, 'mes': 1, 'espérances': 1, 'vol»': 1, 'alors': 1, 'loiseau': 1, 'dit': 1, '«jamais': 1, 'plus»': 1})\n"
     ]
    }
   ],
   "source": [
    "test_count_words.test_count_words_french()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'test_integrations'))\n",
    "import test_integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Clean and tokenize input text\n",
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n",
      "INFO:bws5dk.tokenizers:Text is cleaned and words are in a list\n",
      "INFO:bws5dk.tokenizers:Counting word fequencies...\n",
      "INFO:bws5dk.tokenizers:Done counting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but the raven sitting lonely on the placid bust spoke only that one word as if his soul in that one word he did outpour\n",
      "Counter({'the': 2, 'that': 2, 'one': 2, 'word': 2, 'but': 1, 'raven': 1, 'sitting': 1, 'lonely': 1, 'on': 1, 'placid': 1, 'bust': 1, 'spoke': 1, 'only': 1, 'as': 1, 'if': 1, 'his': 1, 'soul': 1, 'in': 1, 'he': 1, 'did': 1, 'outpour': 1})\n"
     ]
    }
   ],
   "source": [
    "test_integrations.test_integration_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Clean and tokenize input text\n",
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n",
      "INFO:bws5dk.tokenizers:Text is cleaned and words are in a list\n",
      "INFO:bws5dk.tokenizers:Counting word fequencies...\n",
      "INFO:bws5dk.tokenizers:Done counting\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The word `the` appears twice in the sentence, not 2 times",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18536\\1363086267.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_integrations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_integration_fail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\fishy\\OneDrive\\Desktop\\Classes\\Grad_Semester_5\\DS_5111\\bws5dk_DS5111su24_lab_01\\test_integrations\\test_integrations.py\u001b[0m in \u001b[0;36mtest_integration_fail\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"The cleaned text cannot be longer than the input text\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"Word count failed on sample text: {text}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'the'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"The word `the` appears twice in the sentence, not {counts['the']} times\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: The word `the` appears twice in the sentence, not 2 times"
     ]
    }
   ],
   "source": [
    "test_integrations.test_integration_fail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Clean and tokenize input text\n",
      "INFO:bws5dk.tokenizers:Text getting cleaned\n",
      "INFO:bws5dk.tokenizers:Removing punctuation and case sensitivity\n",
      "INFO:bws5dk.tokenizers:Text cleaned\n",
      "INFO:bws5dk.tokenizers:Text has been cleaned\n",
      "INFO:bws5dk.tokenizers:Tokenizing words into list\n",
      "INFO:bws5dk.tokenizers:Text has been tokenzied\n",
      "INFO:bws5dk.tokenizers:Text is cleaned and words are in a list\n",
      "INFO:bws5dk.tokenizers:Counting word fequencies...\n",
      "INFO:bws5dk.tokenizers:Done counting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for word `the`: 729\n"
     ]
    }
   ],
   "source": [
    "test_integrations.test_integration_raven()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
